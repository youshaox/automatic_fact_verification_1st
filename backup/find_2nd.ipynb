{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from collections import Counter\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "# import preprocess\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from math import log, sqrt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "################################################################\n",
    "# ENVIRON\n",
    "################################################################\n",
    "# This is server-gpu\n",
    "server1_homepath = \"/home/ubuntu/workspace/codelab/\"\n",
    "server2_homepath = \"/home/ubuntu/workspace/codelab/\"\n",
    "gpu_homepath = \"/home/shawn/workspace/research/final_codelab/\"\n",
    "jun_homepath = \"/home/junw/workspace/codelab/\"\n",
    "\n",
    "# choose from the server1, server2, gpu, jun.\n",
    "SERVERNAME = 'gpu'\n",
    "HOMEPATH = {'server1':server1_homepath, 'server2':server2_homepath, 'gpu':gpu_homepath, 'jun':jun_homepath}[SERVERNAME]\n",
    "\n",
    "TASKNAME = 'preprocess'\n",
    "\n",
    "## get the path of the wiki files\n",
    "DATAPATH = HOMEPATH + \"data/\"\n",
    "ORGINAL_DATAPATH = DATAPATH +\"orignal/\"\n",
    "INTERMEDIATE_DATAPATH = DATAPATH + \"intermediate/\"\n",
    "FINAL_DATAPATH =  DATAPATH + \"final/\"\n",
    "################################################################\n",
    "# ENVIRON\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [17:38<00:00, 10.23s/it]\n"
     ]
    }
   ],
   "source": [
    "train,dev,test = prepare.get_training_devset_test(ORGINAL_DATAPATH)\n",
    "wiki = prepare.Wiki(DATAPATH, True ,INTERMEDIATE_DATAPATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'alltitles',\n",
       " 'dertitles',\n",
       " 'get_upper_title',\n",
       " 'multi_docs',\n",
       " 'multi_sents',\n",
       " 'search',\n",
       " 'search_tree',\n",
       " 'single_doc',\n",
       " 'single_sent',\n",
       " 'title_tree',\n",
       " 'wiki',\n",
       " 'wiki_titles']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Magic Johnson , Earvin '' Magic '' Johnson Jr. -LRB- born August 14 , 1959 -RRB- is an American retired professional basketball player and current president of basketball operations of the Los Angeles Lakers of the National Basketball Association -LRB- NBA -RRB- .\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.single_sent(['Magic_Johnson', 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare.Wiki"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from math import log, sqrt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import unicodedata\n",
    "import logging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessed_claim_sentence(claim):\n",
    "    claim = unicodedata.normalize('NFC', claim)\n",
    "    claim = claim.replace(':','-COLON-')\n",
    "    claim = claim.replace('-COLON-',' -COLON-')\n",
    "    claim = claim.replace('(','-LRB-')\n",
    "    claim = claim.replace(')','-RRB-')\n",
    "    claim = claim.replace(\"_\",\" \").replace(\"-LRB-\",\"-LRB- \").replace(\"-RRB-\",\" -RRB\")\n",
    "    claim = re.sub('–', '-', claim)\n",
    "    claim = claim.replace(\"`\",\"'\")\n",
    "    claim = claim.replace(\"  \",\" \")\n",
    "    # replaced = re.sub('_-LRB-.*', '', title)\n",
    "    return claim.strip()\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "    \n",
    "def processed(docs, lower = False, stem = False):\n",
    "# processed_docs stores the list of processed docs\n",
    "    processed_docs = []\n",
    "    # vocab contains (term, term id) pairs\n",
    "    vocab = {}\n",
    "    # total_tokens stores the total number of tokens\n",
    "    total_tokens = 0\n",
    "    for raw_doc in docs:\n",
    "        # norm_doc stores the normalized tokens of a doc\n",
    "        norm_doc = []\n",
    "        if stem == True:\n",
    "            tokenized_sentence = raw_doc.split(\" \")\n",
    "        else:\n",
    "            tokenized_sentence = nltk.word_tokenize(raw_doc)##tokenize\n",
    "        for token in tokenized_sentence:\n",
    "            if lower == True:\n",
    "                token = token.lower()\n",
    "            if stem == True:\n",
    "                token = stemmer.stem(token.lower())\n",
    "            if not (token in vocab.keys()):\n",
    "                vocab[token] = len(vocab) ##add into the vocab,len(vocab) will be the id\n",
    "            norm_doc.append(token)\n",
    "            total_tokens += 1\n",
    "        processed_docs.append(norm_doc)\n",
    "        \n",
    "    doc_term_freqs = []\n",
    "    for doc in processed_docs:\n",
    "        doc_term_freqs.append(Counter(doc))\n",
    "        \n",
    "    invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "    \n",
    "    return invindex\n",
    "\n",
    "def doc_eval(setname,predict):\n",
    "    wrong = []\n",
    "    miss = 0\n",
    "    total_len = 0\n",
    "    for key in setname:\n",
    "        total_len += len(predict[key])\n",
    "        for evi in setname[key]['evidence']:\n",
    "            title = unicodedata.normalize('NFC', evi[0])\n",
    "            if title not in predict[key]:\n",
    "                if key not in wrong:\n",
    "                    wrong.append(key)\n",
    "                    miss += 1\n",
    "\n",
    "        #for i in wrong:\n",
    "        #    print(\"==============Claim id: \", i , \" ============\")\n",
    "        #    print(\"Claim : \", setname[i]['claim'])\n",
    "        #    print(\"Target evidence : \", setname[i]['evidence'])\n",
    "        #    print(\"Guess document : \", predict[i])\n",
    "   # return miss,wrong,total_len  \n",
    "    print(\"Recall : \", 1 - miss/len(setname))\n",
    "    print(\"Average Number : \", total_len/len(setname))\n",
    "\n",
    "def query_sim(query, index, k , th = 0):\n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    query = preprocessed_claim_sentence(query)\n",
    "    query = nltk.word_tokenize(query) \n",
    "    for word in query:\n",
    "        word = stemmer.stem(word.lower())\n",
    "        if word in index.vocab:  #The word will not be counted if the word doesn't exist in the vocab,\n",
    "            for i in range(0,len(index.docids(word))):\n",
    "                doc_id = index.docids(word)[i]\n",
    "                dts = 1/sqrt(index.doc_len[doc_id]) * log(1 + index.freqs(word)[i]) * log(index.num_docs() / index.f_t(word)) #calculate the tf-idf score\n",
    "                scores[doc_id] = scores[doc_id] + dts #update the score  \n",
    "    result = []\n",
    "    if th == 0:\n",
    "        return(scores.most_common(k))\n",
    "    else:\n",
    "        for i in scores.most_common(k):\n",
    "            if (i[1] >= th) & (len(result)<3):\n",
    "                result.append(i)\n",
    "            else:\n",
    "                return(result)\n",
    "    return(result)\n",
    "\n",
    "def sent_eval(target,guess):\n",
    "    sen_num = 0\n",
    "    sen_wrong = []\n",
    "    for i in guess:\n",
    "        t = target[i]['evidence']\n",
    "        for e in t:\n",
    "            e[0] = unicodedata.normalize('NFC',e[0])\n",
    "            if not e in guess[i]:\n",
    "                if not i in sen_wrong:\n",
    "                    sen_wrong.append(i)\n",
    "                    sen_num +=1\n",
    "    total_len = 0\n",
    "    for i in guess:\n",
    "        total_len+=len(guess[i])\n",
    "    print(\"Sentence Selection Result\")\n",
    "    print(\"Recall : \",1-sen_num/len(guess))\n",
    "    print(\"Average length : \",total_len/len(guess))\n",
    "    #return 1-sen_num/len(guess),total_len/len(guess)\n",
    "\n",
    "def sentSearch(query,docs,sent_id,wiki = wiki, k = 20, th = 0,  gs = False):\n",
    "    #docs = []\n",
    "    #sent_id = []\n",
    "    #for doc_title in evidence:\n",
    "    #    for doc in wiki.wiki[doc_title]:\n",
    "    #        string = \"\"\n",
    "    #        for i in wiki.wiki[doc_title][doc].split(\",\")[1:]:\n",
    "    #            string += i + \" ,\"\n",
    "    #        docs.append(string[:-1])\n",
    "    #        sent_id.append([doc_title,doc]) \n",
    "    index = processed(docs, lower = True, stem = True)\n",
    "    result = query_sim(query,index,k,th)\n",
    "    docs = []\n",
    "    for i in result:\n",
    "        docs.append(sent_id[i[0]])\n",
    "    return(docs)\n",
    "\n",
    "def ss_grid_search(guess,setname,wiki = wiki):\n",
    "    filt_results_doc = {}\n",
    "    for key in setname:\n",
    "        docs,sent_id = getDoc(guess[key],wiki)\n",
    "        t = sentSearch(setname[key]['claim'],docs,sent_id,k = 100)\n",
    "        filt_results_doc[key] = t\n",
    "    return filt_results_doc\n",
    "\n",
    "def sent_selection_title(TB_docs,setname,wiki=wiki,topk=20,th=0):\n",
    "    filt_results_doc = {}\n",
    "    for key in tqdm(setname):\n",
    "        docs,sent_id = getDoc(TB_docs[key],wiki)\n",
    "        t = sentSearch(setname[key]['claim'],docs,sent_id,k=topk)\n",
    "        filt_results_doc[key] = t\n",
    "    return filt_results_doc\n",
    "\n",
    "def sent_selection_cont(cont_docs,titles,setname,wiki=wiki,topk=100,th=0):\n",
    "    filt_results_doc = {}\n",
    "    for key in tqdm(cont_docs):\n",
    "        docs = []\n",
    "        sent_id = []\n",
    "        cont_title = title_filter_for_cont(cont_docs[key],titles[key])\n",
    "        for sent in cont_title:\n",
    "            string = \"\"\n",
    "            for i in wiki.single_sent(sent).split(\",\")[1:]:\n",
    "                string += i + \" ,\"\n",
    "            docs.append(string[:-1])\n",
    "            sent_id.append(sent)\n",
    "        t = sentSearch(setname[key]['claim'],docs,sent_id,k=topk,th=th)\n",
    "        filt_results_doc[key] = t\n",
    "    return filt_results_doc\n",
    "    \n",
    "def title_filter_for_cont(cont_docs,titles, wiki = wiki):\n",
    "    sent = []\n",
    "    for doc_title in cont_docs:\n",
    "        doc_title = unicodedata.normalize('NFC',doc_title)\n",
    "        for sent_id in wiki.wiki[doc_title]:\n",
    "            doc = wiki.wiki[doc_title][sent_id]\n",
    "            for title in titles:\n",
    "                if title.lower() in doc.lower():\n",
    "                    sent.append([doc_title,sent_id])\n",
    "    return sent\n",
    "\n",
    "def merged_result(sent_sel1,sent_sel2):\n",
    "    merged = {}\n",
    "    for i in sent_sel1:\n",
    "        merged[i] = []\n",
    "        for j in sent_sel1[i]:\n",
    "            merged[i].append(j)\n",
    "        for j in sent_sel2[i]:\n",
    "            if j not in sent_sel1[i]:\n",
    "                merged[i].append(j)\n",
    "    return merged\n",
    "\n",
    "def sentent_selection(TB_docs,cont_docs,setname,titles,wiki=wiki,k = 30, th = 0.5):\n",
    "    TB_title = sent_selection_title(TB_docs,setname)\n",
    "    cont_title = sent_selection_cont(cont_docs,titles,setname,topk = k, th = th)\n",
    "    final_title = merged_result(TB_title,cont_title)\n",
    "    return TB_title\n",
    "\n",
    "def output_senten_result(result,setname,path):\n",
    "    output,claim_id = getoutput(result,setname)\n",
    "    with open(path, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(\"TEST!\")\n",
    "        for i in output:\n",
    "            tsv_writer.writerow(i)\n",
    "    return output,claim_id\n",
    "            \n",
    "def getoutput(result,setname,wiki=wiki):\n",
    "    output = []\n",
    "    claim_id = []\n",
    "    for i in result:\n",
    "        for evi in result[i]:\n",
    "            example = []\n",
    "            claim = setname[i]['claim']\n",
    "            claim = preprocessed_claim_sentence(claim)\n",
    "            claim = unicodedata.normalize('NFC',claim)\n",
    "            example.append(claim)\n",
    "            example.append(wiki.single_sent(evi))\n",
    "            output.append(example)\n",
    "            claim_id.append([i,evi])\n",
    "    return output,claim_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_list(doc_list1,doc_list2,n = 5):\n",
    "    merged_list = []\n",
    "    for doc in doc_list1:\n",
    "        merged_list.append(doc)\n",
    "    for doc in doc_list2:\n",
    "        if doc not in merged_list:\n",
    "            merged_list.append(doc)\n",
    "    return merged_list\n",
    "\n",
    "def getDoc(evidence,wiki):\n",
    "    docs = []\n",
    "    sent_id = []\n",
    "    for doc_title in evidence:\n",
    "        for doc in wiki.wiki[doc_title]:\n",
    "            string = \"\"\n",
    "            for i in wiki.wiki[doc_title][doc].split(\",\")[1:]:\n",
    "                string += i + \" ,\"\n",
    "            docs.append(string[:-1])\n",
    "            sent_id.append([doc_title,doc])\n",
    "    return docs,sent_id\n",
    "\n",
    "def sentSearch(query,docs,sent_id, k = 20, th = 0,  gs = False):\n",
    "    #docs = []\n",
    "    #sent_id = []\n",
    "    #for doc_title in evidence:\n",
    "    #    for doc in wiki.wiki[doc_title]:\n",
    "    #        string = \"\"\n",
    "    #        for i in wiki.wiki[doc_title][doc].split(\",\")[1:]:\n",
    "    #            string += i + \" ,\"\n",
    "    #        docs.append(string[:-1])\n",
    "    #        sent_id.append([doc_title,doc]) \n",
    "    index = processed(docs, lower = True, stem = True)\n",
    "    result = query_sim(query,index,k,th)\n",
    "    docs = []\n",
    "    for i in result:\n",
    "        docs.append(sent_id[i[0]])\n",
    "    return(docs)\n",
    "\n",
    "class Examples:\n",
    "    def __init__(self, examples,wiki):\n",
    "        self.examples = {}\n",
    "        for key in examples:\n",
    "            self.examples[key] = {}\n",
    "            self.examples[key]['claim'] = examples[key]['claim']\n",
    "            self.examples[key]['evidence'] = examples[key]['evidence']\n",
    "            self.examples[key]['label'] = examples[key]['label']\n",
    "        for key in tqdm(self.examples):\n",
    "            claim = self.examples[key]['claim']\n",
    "            self.examples[key]['case_doc'],self.examples[key]['case_title'] = wiki.search(claim,False)\n",
    "            self.examples[key]['uncase_doc'],self.examples[key]['uncase_title'] = wiki.search(claim,True)\n",
    "            if len(self.examples[key]['case_doc']) == 0:\n",
    "                    self.examples[key]['case_doc'] = self.examples[key]['uncase_doc']\n",
    "            self.examples[key]['merged_case_uncase_doc'] = merged_list(self.examples[key]['case_doc'],self.examples[key]['uncase_doc'])\n",
    "            docs,sent_id = getDoc(self.examples[key]['case_doc'],wiki)\n",
    "            self.examples[key]['case_sent'] = sentSearch(claim,docs,sent_id,k=100)\n",
    "            docs,sent_id = getDoc(self.examples[key]['uncase_doc'],wiki)\n",
    "            self.examples[key]['uncase_sent'] = sentSearch(claim,docs,sent_id,k=100)\n",
    "            self.examples[key]['merged_case_uncase_sent'] = merged_list(self.examples[key]['case_sent'],self.examples[key]['uncase_sent'])\n",
    "    \n",
    "    def get_ori(self):\n",
    "        ori = {}\n",
    "        for key in self.examples:\n",
    "            ori[key] = {}\n",
    "            ori[key]['claim'] = self.examples[key]['claim']\n",
    "            ori[key]['evidence'] = self.examples[key]['evidence']\n",
    "            ori[key]['label'] = self.examples[key]['label']\n",
    "        return ori\n",
    "    \n",
    "    def get_uncase_doc(self):\n",
    "        uncase = {}\n",
    "        for key in self.examples:\n",
    "            uncase[key] = self.examples[key]['uncase_doc']\n",
    "        return uncase\n",
    "    \n",
    "    def get_case_doc(self):\n",
    "        case = {}\n",
    "        for key in self.examples:\n",
    "            case[key] = self.examples[key]['case_doc']\n",
    "        return case\n",
    "    \n",
    "    def get_case_sent_list(self,k):\n",
    "        case = {}\n",
    "        for key in self.examples:\n",
    "            kk = min(k,len(self.examples[key]['case_sent']))\n",
    "            case[key] = self.examples[key]['case_sent'][:kk]\n",
    "        return case\n",
    "    \n",
    "    def get_uncase_sent_list(self,k):\n",
    "        uncase = {}\n",
    "        for key in self.examples:\n",
    "            kk = min(k,len(self.examples[key]['uncase_sent']))\n",
    "            uncase[key] = self.examples[key]['uncase_sent'][:kk]\n",
    "        return uncase\n",
    "    \n",
    "    def get_case_sent_full(self,k):\n",
    "        case = {}\n",
    "        for key in self.examples:\n",
    "            kk = min(k,len(self.examples[key]['case_sent']))\n",
    "            case[key] = {}\n",
    "            case[key]['claim'] = self.examples[key]['claim']\n",
    "            case[key]['evidence'] = self.examples[key]['case_sent'][:kk]\n",
    "            case[key]['label'] = self.examples[key]['label']\n",
    "        return case\n",
    "    \n",
    "    def get_case_title(self):\n",
    "        case_title = {}\n",
    "        for key in self.examples:\n",
    "            case_title[key] = self.examples[key]['case_title']\n",
    "            case_title[key] = list(set(case_title[key]))\n",
    "        return case_title\n",
    "    \n",
    "    def get_uncase_sent_full(self,k):\n",
    "        uncase = {}\n",
    "        for key in self.examples:\n",
    "            kk = min(k,len(self.examples[key]['uncase_sent']))\n",
    "            uncase[key] = {}\n",
    "            uncase[key]['claim'] = self.examples[key]['claim']\n",
    "            uncase[key]['evidence'] = self.examples[key]['uncase_sent'][:kk]\n",
    "            uncase[key]['label'] = self.examples[key]['label']\n",
    "        return uncase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [05:53<00:00, 17.79it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_examples = Examples(dev,wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-53f91ecf4e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmerged_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uncase_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'key' is not defined"
     ]
    }
   ],
   "source": [
    "a= merged_list(dev_examples.examples[key]['case_doc'],dev_examples.examples[key]['uncase_doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(INTERMEDIATE_DATAPATH+\"/pylucene_merged_title_content_devset_dict_100.pkl\",'rb') as fp:\n",
    "    merged_devset_dict = pickle.load(fp)\n",
    "cont_dev ={}\n",
    "for key in merged_devset_dict:\n",
    "    cont_dev[key] = merged_devset_dict[key]['matched'][0]\n",
    "    \n",
    "case_title = dev_examples.get_case_title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk(results,k):\n",
    "    new_results = {}\n",
    "    for key in results:\n",
    "        new_results[key] = results[key][:k]\n",
    "    return new_results\n",
    "\n",
    "def get_final(examples,cont_result):\n",
    "    setname = examples.get_ori()\n",
    "    case_title = examples.get_case_title()\n",
    "    filter_cont_sent = sent_selection_cont(cont_result,case_title,setname, topk = 10)\n",
    "    case_sent = examples.get_case_sent_list(20)\n",
    "    uncase_sent = examples.get_uncase_sent_list(5)\n",
    "    merged_uncase_case = merged_result(uncase_sent,case_sent)\n",
    "    final = merged_result(merged_uncase_case,filter_cont_sent)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#直接跑这句话\n",
    "filter_cont_sent = sent_selection_cont(filter_cont,case_title,dev, topk = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cont['91198']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_eval(dev,merged_uncase_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FINAL_DATAPATH + \"cli\",\"w\") as f:\n",
    "    json.dump(cli,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function = get_final(dev_examples,filter_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_eval(dev,test_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dev,claim_id_dev = output_senten_result(final,dev,FINAL_DATAPATH+ \"test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FINAL_DATAPATH + \"claim_id_dev.json\",\"w\") as f:\n",
    "    json.dump(claim_id_dev,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alleval(predicted,actual,gs=False):\n",
    "    NEI = \"NOT ENOUGH INFO\"\n",
    "    correct_label = num_instances = 0\n",
    "    evidence_prec = num_eprec = 0\n",
    "    evidence_recall = num_erec = 0\n",
    "    doc_prec = num_dprec = 0\n",
    "    doc_rec = num_drec = 0\n",
    "\n",
    "    for ident, arecord in actual.items():\n",
    "        precord = predicted[ident]\n",
    "\n",
    "        alabel = arecord['label'].upper()\n",
    "        plabel = precord['label'].upper()\n",
    "        if alabel == plabel:\n",
    "            correct_label += 1\n",
    "        num_instances += 1\n",
    "\n",
    "        if alabel != NEI:\n",
    "            prec = prec_hits = 0\n",
    "            rec = rec_hits = 0\n",
    "\n",
    "            aes = arecord['evidence']\n",
    "            pes = precord['evidence'][:5]\n",
    "            for pe in pes:\n",
    "                if pe in aes:\n",
    "                    prec += 1\n",
    "                prec_hits += 1\n",
    "\n",
    "            for ae in aes:\n",
    "                if ae in pes:\n",
    "                    rec += 1\n",
    "                rec_hits += 1\n",
    "\n",
    "            ads = set(map(lambda ds: ds[0], aes))\n",
    "            last_pd = None\n",
    "            dp = ndp = 0\n",
    "            for pe in pes:\n",
    "                if not last_pd or pe[0] != last_pd:\n",
    "                    if pe[0] in ads:\n",
    "                        dp += 1\n",
    "                    ndp += 1\n",
    "                last_pd = pe[0]\n",
    "\n",
    "            pds = set(map(lambda ds: ds[0], pes))\n",
    "            dr = ndr = 0\n",
    "            for ae in ads:\n",
    "                if ae in pds:\n",
    "                    dr += 1\n",
    "                ndr += 1\n",
    "\n",
    "            if prec_hits > 0:\n",
    "                evidence_prec += float(prec) / prec_hits\n",
    "                num_eprec += 1\n",
    "\n",
    "            if ndp > 0:\n",
    "                doc_prec += float(dp) / ndp\n",
    "                num_dprec += 1\n",
    "\n",
    "            assert rec_hits > 0\n",
    "            evidence_recall += float(rec) / rec_hits\n",
    "            num_erec += 1\n",
    "\n",
    "            assert ndr > 0\n",
    "            doc_rec += float(dr) / ndr\n",
    "            num_drec += 1\n",
    "    accuracy = correct_label / float(num_instances)\n",
    "    precision = evidence_prec / float(num_eprec) if num_eprec != 0 else 0\n",
    "    recall = evidence_recall / float(num_erec) if num_erec != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    doc_precision = doc_prec / float(num_dprec) if num_dprec != 0 else 0\n",
    "    doc_recall = doc_rec / float(num_drec) if num_drec != 0 else 0\n",
    "    doc_f1 = 2 * doc_precision * doc_recall / (doc_precision + doc_recall) if doc_precision + doc_recall > 0 else 0\n",
    "            \n",
    "    doc=[doc_precision,doc_recall,doc_f1]\n",
    "    sent = [precision,recall,f1]\n",
    "    if gs == True:\n",
    "        return doc,sent,accuracy \n",
    "    else:\n",
    "        \n",
    "        print('Label Accuracy', '\\t\\t%.2f%%' % (100 * accuracy))\n",
    "        print('Sentence Precision', '\\t%.2f%%' % (100 * precision))\n",
    "        print('Sentence Recall', '\\t%.2f%%' % (100 * recall))\n",
    "        print('Sentence F1', '\\t\\t%.2f%%' % (100 * f1))\n",
    "        print('Document Precision', '\\t%.2f%%' % (100 * doc_precision))\n",
    "        print('Document Recall', '\\t%.2f%%' % (100 * doc_recall))\n",
    "        print('Document F1', '\\t\\t%.2f%%' % (100 * doc_f1))\n",
    "def read_tsv_result(path):\n",
    "    result = []\n",
    "    with open(path,'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            result.append(row)\n",
    "    return result\n",
    "def evi_list2str(evi):\n",
    "    string = evi[0] + \" \" + str(evi[1])\n",
    "    return(unicodedata.normalize('NFC',string))\n",
    "def get_sent_sel_result(model_output,index, target, th=0.999,k = 5):\n",
    "    result = {}\n",
    "    result_for_claim = {}\n",
    "    for i in target:\n",
    "        result_for_claim[i] = Counter()\n",
    "    for i,res in enumerate(model_output):\n",
    "        ind = index[i][0]\n",
    "        title = evi_list2str(index[i][1])\n",
    "        result_for_claim[ind][title] = float(res[1])    \n",
    "    for i in target:\n",
    "        result[i] = {}\n",
    "        result[i]['claim'] = target[i]['claim']\n",
    "        result[i]['label'] = \"SUPPORTS\"\n",
    "        result[i]['evidence'] = []\n",
    "        titles = result_for_claim[i].most_common(k)\n",
    "        for inx in titles:\n",
    "            if len(result[i]['evidence']) < 1 :\n",
    "                title = [unicodedata.normalize('NFD',inx[0].split(\" \")[0]),int(inx[0].split(\" \")[1])]\n",
    "                result[i]['evidence'].append(title)\n",
    "            else:\n",
    "                if float(inx[1])>th:\n",
    "                        #a = unicodedata.normalize('NFD',inx[0])\n",
    "                        #title = [a.split(\" \")[0],int(a.split(\" \")[1])]\n",
    "                    title = [unicodedata.normalize('NFD',inx[0].split(\" \")[0]),int(inx[0].split(\" \")[1])]\n",
    "                    result[i]['evidence'].append(title)\n",
    "        #for inx in titles:\n",
    "         #   if len(result[i]['evidence'])<1:\n",
    "          #      if float(inx[1]) > 0.99:\n",
    "           #         title = [unicodedata.normalize('NFD',inx[0].split(\" \")[0]),int(inx[0].split(\" \")[1])]\n",
    "            #        result[i]['evidence'].append(title)\n",
    "        #for inx in titles:\n",
    "         #   if len(result[i]['evidence'])<1:\n",
    "          #      if float(inx[1]) > 0.98:\n",
    "            #        title = [unicodedata.normalize('NFD',inx[0].split(\" \")[0]),int(inx[0].split(\" \")[1])]\n",
    "              #      result[i]['evidence'].append(title)\n",
    "            \n",
    "    return result\n",
    "def grid_search_sent(result,index,setname):\n",
    "    th = {}\n",
    "    num = 0.9\n",
    "    max_f1 = 0\n",
    "    th = 0\n",
    "    while num< 1:\n",
    "        after_model = get_sent_sel_result(result,index,setname,th=num)\n",
    "        doc,sent,acc=alleval(after_model,setname,gs=True)\n",
    "        if sent[2]>max_f1:\n",
    "            max_f1 = sent[2]\n",
    "            th = num\n",
    "        num += 0.001\n",
    "    return th\n",
    "def getoutput_final(result,setname,wiki=wiki):\n",
    "    output = []\n",
    "    claim_id = []\n",
    "    for key in result:\n",
    "        example = []\n",
    "        example.append(unicodedata.normalize('NFC',setname[key]['claim']))\n",
    "        example.append(wiki.multi_sents(result[key]['evidence']))\n",
    "        output.append(example)\n",
    "        claim_id.append(key)\n",
    "    return output,claim_id\n",
    "def output_final_test(result,setname,path):\n",
    "    output,claim_id = getoutput_final(result,setname)\n",
    "    with open(path, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(\"TEST!\")\n",
    "        for i in output:\n",
    "            tsv_writer.writerow(i)\n",
    "    return output,claim_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sel_result_list1 = read_tsv_result(\"/home/junw/workspace/Fact_Checking/output/SentSelection/526\"+\"/test_results.tsv\")\n",
    "list_shawn = read_tsv_result(\"/home/junw/workspace/codelab/data/final/output_sentence_prediction_result_test_results.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_re(sent_sel_result_list,dev,claim_id_dev):\n",
    "    result = {}\n",
    "    result_for_claim = {}\n",
    "    for i in dev:\n",
    "        result_for_claim[i] = Counter()\n",
    "    for i,res in enumerate(sent_sel_result_list):\n",
    "        ind = claim_id_dev[i][0]\n",
    "        title = evi_list2str(claim_id_dev[i][1])\n",
    "        result_for_claim[ind][title] = float(res[1])    \n",
    "    for i in dev:\n",
    "        result[i] = {}\n",
    "        result[i]['claim'] = dev[i]['claim']\n",
    "        result[i]['label'] = \"SUPPORTS\"\n",
    "        result[i]['evidence'] = []\n",
    "        titles = result_for_claim[i].most_common(5)\n",
    "    number = {}\n",
    "    new_result = {}\n",
    "    needmore ={}\n",
    "    for i in dev:\n",
    "        new_result[i] = {}\n",
    "        new_result[i]['evidence'] = []\n",
    "        new_result[i]['claim'] = dev[i]['claim']\n",
    "        new_result[i]['label'] = \"SUPPORTS\"\n",
    "        number[i] = [[],[],[]]\n",
    "        titles = result_for_claim[i].most_common(5)\n",
    "        top1 = result_for_claim[i].most_common(1)[0]\n",
    "        for title in titles:\n",
    "            if float(title[1])>=0.999:\n",
    "                number[i][0].append(title[0])\n",
    "            if float(title[1])>=0.995:\n",
    "                number[i][1].append(title[0])\n",
    "            if float(title[1])>=0.95:\n",
    "                number[i][2].append(title[0])\n",
    "        if len(number[i][0])>0:\n",
    "            for title in number[i][0]:\n",
    "                title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                new_result[i]['evidence'].append(title)\n",
    "        else:\n",
    "            if len(number[i][1])>1:\n",
    "                for title in number[i][1]:\n",
    "                    title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                    new_result[i]['evidence'].append(title)\n",
    "            else:\n",
    "                if len(number[i][2])>2:\n",
    "                    for title in number[i][2]:\n",
    "                        title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                        new_result[i]['evidence'].append(title)\n",
    "                else:\n",
    "                    #for title in titles:\n",
    "                     #   if float(title[1])>=0.9:\n",
    "                      #      title= [unicodedata.normalize('NFD',title[0].split(\" \")[0]),int(title[0].split(\" \")[1])]\n",
    "                       #     new_result[i]['evidence'].append(title)\n",
    "                    if top1[1] > 0.5:\n",
    "                        title= [unicodedata.normalize('NFD',top1[0].split(\" \")[0]),int(top1[0].split(\" \")[1])]\n",
    "                        new_result[i]['evidence'].append(title)\n",
    "                    title= [unicodedata.normalize('NFD',top1[0].split(\" \")[0]),int(top1[0].split(\" \")[1])]\n",
    "                    needmore[i] = []\n",
    "                    needmore[i].append(title)\n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr= get_re(sent_sel_result_list1,dev,claim_id_dev)\n",
    "alleval(nr,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_evi = Examples(new_claim_set,wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_claim(needmore,setname):\n",
    "    new_claim = {}\n",
    "    for i in needmore:\n",
    "        new_claim[i] = {}\n",
    "        new_claim[i]['claim'] = setname[i]['claim'] + \" \" + wiki.single_sent(needmore[i][0])\n",
    "        new_claim[i]['label'] = setname[i]['label']\n",
    "        new_claim[i]['evidence'] = setname[i]['evidence']\n",
    "    return new_claim\n",
    "\n",
    "def get_2nd_set(result,claim_id,setname):\n",
    "    new_result,needmore = get_re999(result,setname,claim_id)\n",
    "    new_claim = get_new_claim(needmore,setname)\n",
    "    new_example = Examples(new_claim,wiki)\n",
    "    case = new_example.get_case_sent_list(30)\n",
    "    uncase = new_example.get_uncase_sent_list(6)\n",
    "    merged_c_un = merged_result(case,uncase)\n",
    "    output_dev_2nd,claim_id_dev_2nd = output_senten_result(merged_c_un,new_claim,FINAL_DATAPATH+ \"test_2nd.tsv\")\n",
    "    return output_dev_2nd,claim_id_dev_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_claim = get_new_claim(needmore,dev)\n",
    "sec_evi = Examples(new_claim,wiki)\n",
    "case_2nd = sec_evi.get_case_sent_list(30)\n",
    "sent_eval(new_claim,case_2nd)\n",
    "uncase_2nd = sec_evi.get_uncase_sent_list(6)\n",
    "sent_eval(new_claim,uncase_2nd)\n",
    "merged_list_set = merged_result(case_2nd,uncase_2nd)\n",
    "sent_eval(new_claim,merged_list_set)\n",
    "output_dev_2nd,claim_id_dev_2n2nd = output_senten_result(merged_list_set,new_claim,FINAL_DATAPATH+ \"test_2nd.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sel_result_list_2nd = read_tsv_result(\"/home/junw/workspace/codelab/data/final\"+\"/test_results_2nd.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sel_result_list_2nd = read_tsv_result(\"/home/junw/workspace/codelab/data/final\"+\"/test_results_2nd.tsv\")\n",
    "sent_sel_result_dev_2nd = get_sent_sel_result(sent_sel_result_list_2nd,claim_id_dev_2n2nd,new_claim,th=0.999)\n",
    "alleval(sent_sel_result_dev_2nd,new_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result,needmore = get_re999(sent_sel_result_list1,dev,claim_id_dev)\n",
    "#new_result = get_sent_sel_result(sent_sel_result_list1,claim_id_dev,dev)\n",
    "alleval(new_result,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_dev_2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_2nd = merged_dict(new_result,sent_sel_result_dev_2nd)\n",
    "alleval(final_2nd,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def merged_dict(dic1,dic2):\n",
    "    mer = {}\n",
    "    for i in dic1:\n",
    "        mer[i] = copy.deepcopy(dic1[i])\n",
    "    for i in dic2:\n",
    "        for j in dic2[i]['evidence']:\n",
    "            if j not in mer[i]['evidence']:\n",
    "                mer[i]['evidence'].append(j)\n",
    "    return mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_re999(sent_sel_result_list,dev,claim_id_dev,th = 0.999):\n",
    "    result = {}\n",
    "    result_for_claim = {}\n",
    "    for i in dev:\n",
    "        result_for_claim[i] = Counter()\n",
    "    for i,res in enumerate(sent_sel_result_list):\n",
    "        ind = claim_id_dev[i][0]\n",
    "        title = evi_list2str(claim_id_dev[i][1])\n",
    "        result_for_claim[ind][title] = float(res[1])    \n",
    "    for i in dev:\n",
    "        result[i] = {}\n",
    "        result[i]['claim'] = dev[i]['claim']\n",
    "        result[i]['label'] = \"SUPPORTS\"\n",
    "        result[i]['evidence'] = []\n",
    "        titles = result_for_claim[i].most_common(5)\n",
    "    number = {}\n",
    "    new_result = {}\n",
    "    needmore ={}\n",
    "    for i in dev:\n",
    "        new_result[i] = {}\n",
    "        new_result[i]['evidence'] = []\n",
    "        new_result[i]['claim'] = dev[i]['claim']\n",
    "        new_result[i]['label'] = \"SUPPORTS\"\n",
    "        number[i] = [[],[],[]]\n",
    "        titles = result_for_claim[i].most_common(5)\n",
    "        top1 = result_for_claim[i].most_common(1)[0]\n",
    "        for title in titles:\n",
    "            if float(title[1])>=th:\n",
    "                number[i][0].append(title[0])\n",
    "        if len(number[i][0])>0:\n",
    "            for title in number[i][0]:\n",
    "                title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                new_result[i]['evidence'].append(title)\n",
    "        else:\n",
    "            title= [unicodedata.normalize('NFD',top1[0].split(\" \")[0]),int(top1[0].split(\" \")[1])]\n",
    "            #new_result[i]['evidence'].append(title)\n",
    "            needmore[i] = []\n",
    "            needmore[i].append(title)\n",
    "    return new_result,needmore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result,needmore = get_re999(sent_sel_result_list1,dev,claim_id_dev,th=0.999)\n",
    "#new_result = get_sent_sel_result(sent_sel_result_list1,claim_id_dev,dev)\n",
    "alleval(new_result,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sent_sel_result_list_2nd = read_tsv_result(\"/home/junw/workspace/codelab/data/final\"+\"/test_results_2nd.tsv\")\n",
    "sent_sel_result_dev_2nd,nomore = get_re999(sent_sel_result_list_2nd,new_claim,claim_id_dev_2n2nd,th=0.9996)\n",
    "alleval(sent_sel_result_dev_2nd,new_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_re = merged_dict(new_result,sent_sel_result_dev_2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleval(final_re,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    result = {}\n",
    "    result_for_claim = {}\n",
    "    result_for_claim_2nd = {}\n",
    "    merged_result = {}\n",
    "    for i in dev:\n",
    "        result_for_claim[i] = Counter()\n",
    "    for i in dev:\n",
    "        result_for_claim_2nd[i] = Counter()\n",
    "        merged_result[i] = Counter()\n",
    "    for i,res in enumerate(sent_sel_result_list1):\n",
    "        ind = claim_id_dev[i][0]\n",
    "        title = evi_list2str(claim_id_dev[i][1])\n",
    "        result_for_claim[ind][title] = float(res[1])\n",
    "        merged_result[ind][title] = float(res[1])\n",
    "    for i,res in enumerate(sent_sel_result_list_2nd):\n",
    "        ind = claim_id_dev_2n2nd[i][0]\n",
    "        title = evi_list2str(claim_id_dev_2n2nd[i][1])\n",
    "        result_for_claim_2nd[ind][title] = float(res[1])\n",
    "        merged_result[ind][title] = max(merged_result[ind][title],result_for_claim[ind].most_common(1)[0][1]*float(res[1]))\n",
    "    #for i in dev:\n",
    "      #  if result_for_claim_2nd[i].most_common(1) != []:\n",
    "        #    title  = result_for_claim_2nd[i].most_common(1)[0][0]\n",
    "          #  sco = result_for_claim_2nd[i].most_common(1)[0][1]\n",
    "            #merged_result[i][title] = max(merged_result[i][title],result_for_claim[i].most_common(1)[0][1]*sco)\n",
    "    number = {}\n",
    "    new_result = {}\n",
    "    needmore ={}\n",
    "    for i in dev:\n",
    "        new_result[i] = {}\n",
    "        new_result[i]['evidence'] = []\n",
    "        new_result[i]['claim'] = dev[i]['claim']\n",
    "        new_result[i]['label'] = \"SUPPORTS\"\n",
    "        number[i] = [[],[],[]]\n",
    "        titles = result_for_claim[i].most_common(5)\n",
    "        top1 = result_for_claim[i].most_common(1)[0]\n",
    "        titles_merged = merged_result[i].most_common(5)\n",
    "        for title in titles_merged:\n",
    "            if float(title[1])>=0.999:\n",
    "                number[i][0].append(title[0])\n",
    "            if float(title[1])>=0.996:\n",
    "                number[i][1].append(title[0])\n",
    "            if float(title[1])>=0.99:\n",
    "                number[i][2].append(title[0])\n",
    "        if len(number[i][0])>0:\n",
    "            for title in number[i][0]:\n",
    "                title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                new_result[i]['evidence'].append(title)\n",
    "        else:\n",
    "            if len(number[i][1])>1:\n",
    "                for title in number[i][1]:\n",
    "                    title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                    new_result[i]['evidence'].append(title)\n",
    "            else:\n",
    "                if len(number[i][2])>2:\n",
    "                    for title in number[i][2]:\n",
    "                        title= [unicodedata.normalize('NFD',title.split(\" \")[0]),int(title.split(\" \")[1])]\n",
    "                        new_result[i]['evidence'].append(title)\n",
    "                else:\n",
    "                    if top1[1] > 0.9:\n",
    "                        title= [unicodedata.normalize('NFD',top1[0].split(\" \")[0]),int(top1[0].split(\" \")[1])]\n",
    "                        new_result[i]['evidence'].append(title)\n",
    "                   #  for title in titles_merged:   \n",
    "                    #    if float(title[1]) >= 0.9:\n",
    "                       #     title= [unicodedata.normalize('NFD',top1[0].split(\" \")[0]),int(top1[0].split(\" \")[1])]\n",
    "                         #   new_result[i]['evidence'].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleval(new_result,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr  = get_sent_sel_result(sent_sel_result_list1,claim_id_dev,dev,th=0.999)\n",
    "alleval(nr,dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alleval(predicted,actual,gs=False):\n",
    "    NEI = \"NOT ENOUGH INFO\"\n",
    "    correct_label = num_instances = 0\n",
    "    evidence_prec = num_eprec = 0\n",
    "    evidence_recall = num_erec = 0\n",
    "    doc_prec = num_dprec = 0\n",
    "    doc_rec = num_drec = 0\n",
    "\n",
    "    for ident, arecord in actual.items():\n",
    "        precord = predicted[ident]\n",
    "\n",
    "        alabel = arecord['label'].upper()\n",
    "        plabel = precord['label'].upper()\n",
    "        if alabel == plabel:\n",
    "            correct_label += 1\n",
    "        num_instances += 1\n",
    "\n",
    "        if alabel != NEI:\n",
    "            prec = prec_hits = 0\n",
    "            rec = rec_hits = 0\n",
    "\n",
    "            aes = arecord['evidence']\n",
    "            pes = precord['evidence'][:5]\n",
    "            for pe in pes:\n",
    "                if pe in aes:\n",
    "                    prec += 1\n",
    "                prec_hits += 1\n",
    "\n",
    "            for ae in aes:\n",
    "                if ae in pes:\n",
    "                    rec += 1\n",
    "                rec_hits += 1\n",
    "\n",
    "            ads = set(map(lambda ds: ds[0], aes))\n",
    "            last_pd = None\n",
    "            dp = ndp = 0\n",
    "            for pe in pes:\n",
    "                if not last_pd or pe[0] != last_pd:\n",
    "                    if pe[0] in ads:\n",
    "                        dp += 1\n",
    "                    ndp += 1\n",
    "                last_pd = pe[0]\n",
    "\n",
    "            pds = set(map(lambda ds: ds[0], pes))\n",
    "            dr = ndr = 0\n",
    "            for ae in ads:\n",
    "                if ae in pds:\n",
    "                    dr += 1\n",
    "                ndr += 1\n",
    "\n",
    "            if prec_hits > 0:\n",
    "                evidence_prec += float(prec) / prec_hits\n",
    "                num_eprec += 1\n",
    "\n",
    "            if ndp > 0:\n",
    "                doc_prec += float(dp) / ndp\n",
    "                num_dprec += 1\n",
    "\n",
    "            assert rec_hits > 0\n",
    "            evidence_recall += float(rec) / rec_hits\n",
    "            num_erec += 1\n",
    "\n",
    "            assert ndr > 0\n",
    "            doc_rec += float(dr) / ndr\n",
    "            num_drec += 1\n",
    "    accuracy = correct_label / float(num_instances)\n",
    "    precision = evidence_prec / float(num_eprec) if num_eprec != 0 else 0\n",
    "    recall = evidence_recall / float(num_erec) if num_erec != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    doc_precision = doc_prec / float(num_dprec) if num_dprec != 0 else 0\n",
    "    doc_recall = doc_rec / float(num_drec) if num_drec != 0 else 0\n",
    "    doc_f1 = 2 * doc_precision * doc_recall / (doc_precision + doc_recall) if doc_precision + doc_recall > 0 else 0\n",
    "            \n",
    "    doc=[doc_precision,doc_recall,doc_f1]\n",
    "    sent = [precision,recall,f1]\n",
    "    if gs == True:\n",
    "        return doc,sent,accuracy \n",
    "    else:\n",
    "        \n",
    "        print('Label Accuracy', '\\t\\t%.2f%%' % (100 * accuracy))\n",
    "        print('Sentence Precision', '\\t%.2f%%' % (100 * precision))\n",
    "        print('Sentence Recall', '\\t%.2f%%' % (100 * recall))\n",
    "        print('Sentence F1', '\\t\\t%.2f%%' % (100 * f1))\n",
    "        print('Document Precision', '\\t%.2f%%' % (100 * doc_precision))\n",
    "        print('Document Recall', '\\t%.2f%%' % (100 * doc_recall))\n",
    "        print('Document F1', '\\t\\t%.2f%%' % (100 * doc_f1))\n",
    "def read_tsv_result(path):\n",
    "    result = []\n",
    "    with open(path,'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            result.append(row)\n",
    "    return result\n",
    "def evi_list2str(evi):\n",
    "    string = evi[0] + \" \" + str(evi[1])\n",
    "    return(unicodedata.normalize('NFC',string))\n",
    "def get_sent_sel_result(model_output,index, target, th=0.999,k = 5):\n",
    "    result = {}\n",
    "    result_for_claim = {}\n",
    "    for i in target:\n",
    "        result_for_claim[i] = Counter()\n",
    "    for i,res in enumerate(model_output):\n",
    "        ind = index[i][0]\n",
    "        title = evi_list2str(index[i][1])\n",
    "        result_for_claim[ind][title] = float(res[1])    \n",
    "    for i in target:\n",
    "        result[i] = {}\n",
    "        result[i]['claim'] = target[i]['claim']\n",
    "        result[i]['label'] = \"SUPPORTS\"\n",
    "        result[i]['evidence'] = []\n",
    "        titles = result_for_claim[i].most_common(k)\n",
    "        for inx in titles:\n",
    "            if len(result[i]['evidence']) < 1 :\n",
    "                title = [unicodedata.normalize('NFD',inx[0].split(\" \")[0]),int(inx[0].split(\" \")[1])]\n",
    "                result[i]['evidence'].append(title)\n",
    "            else:\n",
    "                if float(inx[1])>th:\n",
    "                    #a = unicodedata.normalize('NFD',inx[0])\n",
    "                    #title = [a.split(\" \")[0],int(a.split(\" \")[1])]\n",
    "                    title = [unicodedata.normalize('NFD',inx[0].split(\" \")[0]),int(inx[0].split(\" \")[1])]\n",
    "                    result[i]['evidence'].append(title)\n",
    "    return result\n",
    "def grid_search_sent(result,index,setname):\n",
    "    th = {}\n",
    "    num = 0.9\n",
    "    max_f1 = 0\n",
    "    th = 0\n",
    "    while num< 1:\n",
    "        after_model = get_sent_sel_result(result,index,setname,th=num)\n",
    "        doc,sent,acc=alleval(after_model,setname,gs=True)\n",
    "        if sent[2]>max_f1:\n",
    "            max_f1 = sent[2]\n",
    "            th = num\n",
    "        num += 0.001\n",
    "    return th\n",
    "def getoutput_final(result,setname,wiki=wiki):\n",
    "    output = []\n",
    "    claim_id = []\n",
    "    for key in result:\n",
    "        example = []\n",
    "        example.append(preprocessed_claim_sentence(unicodedata.normalize('NFC',setname[key]['claim'])))\n",
    "        example.append(wiki.multi_sents(result[key]['evidence']))\n",
    "        output.append(example)\n",
    "        claim_id.append(key)\n",
    "    return output,claim_id\n",
    "def output_final_test(result,setname,path):\n",
    "    output,claim_id = getoutput_final(result,setname)\n",
    "    with open(path, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(\"TEST!\")\n",
    "        for i in output:\n",
    "            tsv_writer.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(res):\n",
    "    res_new = []\n",
    "    for i in res:\n",
    "        res_new.append(float(i))\n",
    "    num = res_new.index(max(res_new))\n",
    "    if num==0:\n",
    "        return \"REFUTES\"\n",
    "    if num ==1:\n",
    "        return \"SUPPORTS\"\n",
    "    if num == 2:\n",
    "        return \"NOT ENOUGH INFO\"\n",
    "def get_final_result(sen_sel_result,final_result,index):\n",
    "    for i,res in enumerate(final_result):\n",
    "        claim_id = index[i]\n",
    "        sen_sel_result[claim_id]['label'] = getLabel(res)\n",
    "    return sen_sel_result\n",
    "\n",
    "def final_output(result,path):\n",
    "    new_final = {}\n",
    "    for i in result:\n",
    "        new_final[i] = {}\n",
    "        new_final[i]['claim'] = unicodedata.normalize('NFD', result[i]['claim'])\n",
    "        new_final[i]['label'] = unicodedata.normalize('NFD', result[i]['label'])\n",
    "        new_evi = []\n",
    "        for evi in result[i]['evidence']:\n",
    "            new_evi.append([unicodedata.normalize('NFD', evi[0]),evi[1]])\n",
    "        new_final[i]['evidence'] = new_evi\n",
    "    with open(path,\"w\") as f:\n",
    "        json.dump(new_final,f)\n",
    "    return new_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr= get_re(sent_sel_result_list1,dev,claim_id_dev)\n",
    "alleval(nr,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr= get_re(sent_sel_result_list1,dev,claim_id_dev)\n",
    "alleval(nr,dev)\n",
    "output_final, cli = getoutput_final(nr,dev)\n",
    "output_final_test(nr,dev,FINAL_DATAPATH + \"final_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_tsv = read_tsv_result(FINAL_DATAPATH+\"/test_results_final.tsv\")\n",
    "final_result = get_final_result(nr,final_result_tsv,cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = get_final_result(nr,final_result_tsv,cli)\n",
    "alleval(final_result,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highrecall = get_sent_sel_result(sent_sel_result_list1,claim_id_dev,dev,th = 0.90)\n",
    "alleval(highrecall,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_final_rc, cli_rc = getoutput_final(highrecall,dev)\n",
    "output_final_test(highrecall,dev,FINAL_DATAPATH + \"final_test_reca.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_tsv_high = read_tsv_result(FINAL_DATAPATH+\"/result_highrecall.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_high = get_final_result(nr,final_result_tsv_high,cli_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.wiki['Floppy_disk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output(final_result_high,FINAL_DATAPATH+\"/testoutput.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleval(final_result_high,dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei = []\n",
    "for i in final_result_high:\n",
    "    if (final_result_high[i]['label'] == \"REFUTES\") & (dev[i]['label'] != \"REFUTES\"):\n",
    "        for j in dev[i]['e']\n",
    "        nei.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_rc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refutes = []\n",
    "for i in final_result_high:\n",
    "    if (final_result_high[i]['label'] != \"REFUTES\") & (dev[i]['label'] == \"REFUTES\"):\n",
    "        refutes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(refutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightevi(evi1,evi2):\n",
    "    for i in evi1:\n",
    "        if i not in evi2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=0\n",
    "sw = 0\n",
    "r=0\n",
    "rw = 0\n",
    "ne = 0\n",
    "nw = 0\n",
    "a = [0,0,0]\n",
    "for i in final_result_high:\n",
    "    if final_result_high[i]['label'] == \"SUPPORTS\":\n",
    "        if dev[i]['label'] != \"SUPPORTS\":\n",
    "            if rightevi(dev[i]['evidence'],final_result_high[i]['evidence']):\n",
    "                a[0]+=1\n",
    "            sw+=1\n",
    "        s+=1\n",
    "    if final_result_high[i]['label'] == \"REFUTES\":\n",
    "        if dev[i]['label'] != \"REFUTES\":\n",
    "            if rightevi(dev[i]['evidence'],final_result_high[i]['evidence']):\n",
    "                a[1]+=1\n",
    "            rw+=1\n",
    "        r+=1\n",
    "    if final_result_high[i]['label'] == \"NOT ENOUGH INFO\":\n",
    "        if dev[i]['label'] != \"NOT ENOUGH INFO\":\n",
    "            if rightevi(dev[i]['evidence'],final_result_high[i]['evidence']):\n",
    "                a[2]+=1\n",
    "            nw+=1\n",
    "        ne+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0],a[1],a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testrecall = {}\n",
    "for i in final_result_high:\n",
    "    testrecall[i] = final_result_high[i]['evidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_eval(dev,testrecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        with open('xx.tsv', 'wt') as fp:\n",
    "            tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "            for row in examples:\n",
    "                tsv_writer.writerow(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [claim, content, 0/1/2]\n",
    "class ClassificationGenerator:\n",
    "    def __init__(self, wiki, train_examples):\n",
    "        self.wiki = wiki\n",
    "        self.train_examples = train_examples\n",
    "\n",
    "    def output2file(self, _lists):\n",
    "#         assert nei_evidences_dataset is not None\n",
    "        with open('/home/shawn/workspace/research/final_codelab/shawn/input/classification/train.tsv', 'wt') as fp:\n",
    "            tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "#             tsv_writer.writerow(\"TEST!\")\n",
    "            for _list in _lists:\n",
    "#                 claim = preprocessed_claim_sentence(_dict['claim'])\n",
    "#                 claim = unicodedata.normalize('NFC', claim)\n",
    "#                 output_list = [claim, _dict['evidence_sentence']]\n",
    "                tsv_writer.writerow(_list)\n",
    "        \n",
    "    def run(self):\n",
    "        # [claim, doc, label]\n",
    "        _lists = list()\n",
    "        sup_cnt = 0\n",
    "        ref_cnt = 0\n",
    "        nei_cnt = 0\n",
    "        for key in self.train_examples.examples.keys():\n",
    "            info_dict = self.train_examples.examples[key]\n",
    "            label = info_dict['label']\n",
    "            claim = info_dict['claim']\n",
    "            claim = preprocessed_claim_sentence(claim)\n",
    "            claim = unicodedata.normalize('NFC', claim)\n",
    "            # 0 refutes 1 supports 2 nei\n",
    "            t_label = None\n",
    "            doc = \"\"\n",
    "            if label == 'NOT ENOUGH INFO':\n",
    "                nei_cnt +=1\n",
    "                t_label = 2\n",
    "                length = random.choices(population=[1, 2, 3, 4, 5], weights=[0.4, 0.3, 0.1, 0.1, 0.1], k=1)[0]\n",
    "                case_sents = info_dict['case_sent'][0:length]\n",
    "                doc = self.wiki.multi_sents(case_sent)\n",
    "                _lists.append([claim, doc, str(t_label)])\n",
    "            elif label == 'SUPPORTS' and sup_cnt<=40000:\n",
    "                sup_cnt += 1 \n",
    "                t_label = 1\n",
    "                evidence_sents = info_dict['evidence'][0:5]\n",
    "                doc = self.wiki.multi_sents(evidence_sents)\n",
    "                _lists.append([claim, doc, str(t_label)])\n",
    "            elif label == 'REFUTES':\n",
    "                # refutes\n",
    "                ref_cnt += 1\n",
    "                t_label = 0\n",
    "                evidence_sents = info_dict['evidence'][0:5]\n",
    "                doc = self.wiki.multi_sents(evidence_sents)\n",
    "                _lists.append([claim, doc, str(t_label)])\n",
    "#             assert t_label is not None\n",
    "#             assert doc is not \"\"\n",
    "#             _lists.append([claim, doc, str(t_label)])\n",
    "        self.output2file(_lists)\n",
    "        print(sup_cnt, ref_cnt, nei_cnt)\n",
    "        return _lists\n",
    "classificationGenerator = ClassificationGenerator(wiki, train_examples)\n",
    "_lists = classificationGenerator.run()\n",
    "print(len(_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0\n",
    "for i in train_output:\n",
    "    if i[2] == '1':\n",
    "        s+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "s = 0\n",
    "for i in train_output:\n",
    "    if i[2] != '1':\n",
    "        new_output.append(i)\n",
    "    else:\n",
    "        if s<=50000:\n",
    "            new_output.append(i)\n",
    "            s+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('balance_train.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    for i in new_output:\n",
    "        tsv_writer.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "r= 0\n",
    "nei = 0\n",
    "for i in new_output:\n",
    "    if i[2] == '0':\n",
    "        r+=1\n",
    "    if i[2] == '1':\n",
    "        s+=1\n",
    "    if i[2] == '2':\n",
    "        nei+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s,r,nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s/(s+r+nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r/(s+r+nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei/(s+r+nei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50001/(29775+35639+50001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = read_tsv_result(\"/home/junw/workspace/Fact_Checking/output/Recog/final518/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\" in train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_output:\n",
    "    if \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.',\n",
    "  'Fox Broadcasting Company , The Fox Broadcasting Company -LRB- often shortened to Fox and stylized as FOX -RRB- is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox . Nikolaj Coster-Waldau , He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam -LRB- 2008 -RRB- , as well as appearing as Frank Pike in the 2009 Fox television film Virtuality , originally intended as a pilot .',\n",
    "  '1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= ['Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'Fox Broadcasting Company , The Fox Broadcasting Company -LRB-   often shortened to Fox and stylized as FOX  -RRB is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox . Nikolaj Coster-Waldau , He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam -LRB-   2008  -RRB , as well as appearing as Frank Pike in the 2009 Fox television film Virtuality , originally intended as a pilot .', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] == b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.title_tree['fox']['broadcasting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.title_tree['university']['of']['sydney']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.title_tree['university']['of']['melbourne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.title_tree['fox']['<$>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.title_tree['fox']['broadcasting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples.examples['40226']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.dertitles('heavy metal music')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.dertitles('fox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<prepare.Wiki at 0x7fe28ab38c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.wiki.single_sent([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.title_tree['the']['game']['of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
